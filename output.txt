NeoXArgs.from_ymls() ['pythia-160m-deduped.yml']
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 1
-------------------- arguments --------------------
  attention_config ................ ['flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash']updated
  attention_dropout ............... 0...........................updated
  batch_size ...................... 1...........................updated
  bias_gelu_fusion ................ True........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 1000........................updated
  clip_grad ....................... 1.0.........................updated
  config_files .................... {'pythia-160m-deduped.yml': '{\n  # parallelism settings\n  "pipe-parallel-size": 1,\n  "model-parallel-size": 1,\n\n  # model settings\n  "num-layers": 12,\n  "hidden-size": 768,\n  "num-attention-heads": 12,\n  "seq-length": 2048,\n  "max-position-embeddings": 2048,\n  "pos-emb": "rotary",\n  "rotary-pct": 0.25,\n  "no-weight-tying": true,\n  "gpt-j-residual": true,\n  "output-layer-parallelism": "column",\n  \n  "attention-config": [[["flash"], 12]],\n\n  "scaled-upper-triang-masked-softmax-fusion": true,\n  "bias-gelu-fusion": true,\n\n  # init methods\n  "init_method": "small_init",\n  "output_layer_init_method": "wang_init",\n\n  "optimizer": {\n    "type": "Adam",\n    "params": {\n      "lr": 0.0006,\n      "betas": [0.9, 0.95],\n      "eps": 1.0e-8\n    }\n  },\n  "min_lr": 0.00006,\n\n  "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": true,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n  # batch size (trained on 32 gpus)\n  "train_micro_batch_size_per_gpu": 1,\n  #@@RV modified from 32 to 1 above\n  # "gas": 1,\n  #@@RV commented out \n  "data-impl": "mmap",\n  "num_workers": 1,\n  #@@RV modified from 1 to 2 above\n\n  # activation checkpointing\n  "checkpoint-activations": true,\n  "checkpoint-num-layers": 1,\n  "partition-activations": true,\n  "synchronize-each-layer": true,\n\n  # regularization\n  "gradient_clipping": 1.0,\n  "weight-decay": 0.1,\n  "hidden-dropout": 0,\n  "attention-dropout": 0,\n\n  # precision settings\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 12,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  # "train-iters": 143000,\n  "train-iters": 10000,\n  "lr-decay-iters": 143000,\n  "distributed-backend": "nccl",\n  "lr-decay-style": "cosine",\n  "warmup": 0.01,\n  "checkpoint-factor": 1000,\n  "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],\n  "eval-interval": 40000,\n  "eval-iters": 10,\n\n  "log-interval": 10,\n  "steps_per_print": 10,\n  "wall_clock_breakdown": true,\n\n  # "train-data-paths": ["/fsx/pile_deduped/pile_0.87_deduped_text_document"],\n  # "valid-data-paths": ["/fsx/pile_deduped/pile_0.87_deduped_text_document"],\n  # "test-data-paths": ["/fsx/pile_deduped/pile_0.87_deduped_text_document"],\n  "train-data-paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"],\n  "valid-data-paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"],\n  "test-data-paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"],\n\n  "tokenizer-type": "HFTokenizer",\n  "vocab-file": "/workspace/20B_tokenizer.json",\n\n  "launcher": "slurm",\n  "deepspeed_slurm": false,\n\n  "save": "checkpoints/model_160m",\n  "load": "checkpoints/model_160m",\n  "checkpoint_validation_with_forward_pass": False,\n}\n'}updated
  data_impl ....................... mmap........................updated
  dynamic_loss_scale .............. True........................updated
  eval_interval ................... 40000.......................updated
  eval_iters ...................... 10..........................updated
  extra_save_iters ................ [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
  gas ............................. 1...........................updated
  global_num_gpus ................. 1...........................updated
  gpt_j_residual .................. True........................updated
  gradient_clipping ............... 1.0.........................updated
  hidden_dropout .................. 0...........................updated
  hidden_size ..................... 768.........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  launcher ........................ slurm.......................updated
  load ............................ checkpoints/model_160m......updated
  log_interval .................... 10..........................updated
  lr .............................. 0.0006......................updated
  lr_decay_iters .................. 143000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  min_lr .......................... 6e-05.......................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 12..........................updated
  num_layers ...................... 12..........................updated
  num_workers ..................... 1...........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  output_layer_parallelism ........ column......................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  rotary_pct ...................... 0.25........................updated
  save ............................ checkpoints/model_160m......updated
  save_iters ...................... [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]updated
  scaled_upper_triang_masked_softmax_fusion  True...............updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  test_data_paths ................. ['/pythia_pile_idxmaps/pile_0.87_deduped_text_document']updated
  test_data_weights ............... [1.0].......................updated
  text_gen_type ................... unconditional...............updated
  tokenizer_type .................. HFTokenizer.................updated
  train_batch_size ................ 1...........................updated
  train_data_paths ................ ['/pythia_pile_idxmaps/pile_0.87_deduped_text_document']updated
  train_data_weights .............. [1.0].......................updated
  train_iters ..................... 10000.......................updated
  train_micro_batch_size_per_gpu .. 1...........................updated
  user_script ..................... train.py....................updated
  valid_data_paths ................ ['/pythia_pile_idxmaps/pile_0.87_deduped_text_document']updated
  valid_data_weights .............. [1.0].......................updated
  vocab_file ...................... /workspace/20B_tokenizer.jsonupdated
  wall_clock_breakdown ............ True........................updated
  wandb_group ..................... ZBQA5Wo9gxCEZn2i7LTWsv_muey74v1updated
  weight_decay .................... 0.1.........................updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  activation ...................... gelu........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_softmax_in_fp32 ....... False.......................default
  bias_dropout_fusion ............. False.......................default
  char_level_ppl .................. False.......................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  comment ......................... None........................default
  contiguous_checkpointing ........ False.......................default
  data_path ....................... None........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dump_state ...................... False.......................default
  eod_mask_loss ................... False.......................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  exit_interval ................... None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  git_hash ........................ 71df4d50....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_tied ...................... False.......................default
  gradient_accumulation_steps ..... 1...........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  iteration ....................... None........................default
  keep_last_n_checkpoints ......... None........................default
  layernorm_epsilon ............... 1e-05.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_dir ......................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  merge_file ...................... None........................default
  min_scale ....................... 1.0.........................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  override_lr_scheduler ........... False.......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  prescale_gradients .............. False.......................default
  profile_backward ................ False.......................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rotary_emb_base ................. 10000.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  scaled_masked_softmax_fusion .... False.......................default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  short_seq_prob .................. 0.1.........................default
  soft_prompt_tuning .............. None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  temperature ..................... 0.0.........................default
  tensorboard_dir ................. None........................default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_cpu_initialization .......... False.......................default
  use_shared_fs ................... True........................default
  use_wandb ....................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weighted_sampler_alpha .......... 0.3.........................default
  world_size ...................... None........................default
  zero_allow_untested_optimizer ... False.......................default
---------------- end of arguments ----------------
===========================================================================
['--launcher', 'slurm', 'train.py', '--deepspeed_config', '{"train_batch_size": 1, "train_micro_batch_size_per_gpu": 1, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "gradient_clipping": 1.0, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true}', '--megatron_config', '{"launcher": "slurm", "train_batch_size": 1, "train_micro_batch_size_per_gpu": 1, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "gradient_clipping": 1.0, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "output_layer_parallelism": "column", "lr_decay_style": "cosine", "lr_decay_iters": 143000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"], "test_data_paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"], "valid_data_paths": ["/pythia_pile_idxmaps/pile_0.87_deduped_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "checkpoints/model_160m", "config_files": {"pythia-160m-deduped.yml": "{\\n  # parallelism settings\\n  \\"pipe-parallel-size\\": 1,\\n  \\"model-parallel-size\\": 1,\\n\\n  # model settings\\n  \\"num-layers\\": 12,\\n  \\"hidden-size\\": 768,\\n  \\"num-attention-heads\\": 12,\\n  \\"seq-length\\": 2048,\\n  \\"max-position-embeddings\\": 2048,\\n  \\"pos-emb\\": \\"rotary\\",\\n  \\"rotary-pct\\": 0.25,\\n  \\"no-weight-tying\\": true,\\n  \\"gpt-j-residual\\": true,\\n  \\"output-layer-parallelism\\": \\"column\\",\\n  \\n  \\"attention-config\\": [[[\\"flash\\"], 12]],\\n\\n  \\"scaled-upper-triang-masked-softmax-fusion\\": true,\\n  \\"bias-gelu-fusion\\": true,\\n\\n  # init methods\\n  \\"init_method\\": \\"small_init\\",\\n  \\"output_layer_init_method\\": \\"wang_init\\",\\n\\n  \\"optimizer\\": {\\n    \\"type\\": \\"Adam\\",\\n    \\"params\\": {\\n      \\"lr\\": 0.0006,\\n      \\"betas\\": [0.9, 0.95],\\n      \\"eps\\": 1.0e-8\\n    }\\n  },\\n  \\"min_lr\\": 0.00006,\\n\\n  \\"zero_optimization\\": {\\n    \\"stage\\": 1,\\n    \\"allgather_partitions\\": true,\\n    \\"allgather_bucket_size\\": 500000000,\\n    \\"overlap_comm\\": true,\\n    \\"reduce_scatter\\": true,\\n    \\"reduce_bucket_size\\": 500000000,\\n    \\"contiguous_gradients\\": true,\\n    \\"cpu_offload\\": false\\n  },\\n\\n  # batch size (trained on 32 gpus)\\n  \\"train_micro_batch_size_per_gpu\\": 1,\\n  #@@RV modified from 32 to 1 above\\n  # \\"gas\\": 1,\\n  #@@RV commented out \\n  \\"data-impl\\": \\"mmap\\",\\n  \\"num_workers\\": 1,\\n  #@@RV modified from 1 to 2 above\\n\\n  # activation checkpointing\\n  \\"checkpoint-activations\\": true,\\n  \\"checkpoint-num-layers\\": 1,\\n  \\"partition-activations\\": true,\\n  \\"synchronize-each-layer\\": true,\\n\\n  # regularization\\n  \\"gradient_clipping\\": 1.0,\\n  \\"weight-decay\\": 0.1,\\n  \\"hidden-dropout\\": 0,\\n  \\"attention-dropout\\": 0,\\n\\n  # precision settings\\n  \\"fp16\\": {\\n    \\"fp16\\": true,\\n    \\"enabled\\": true,\\n    \\"loss_scale\\": 0,\\n    \\"loss_scale_window\\": 1000,\\n    \\"initial_scale_power\\": 12,\\n    \\"hysteresis\\": 2,\\n    \\"min_loss_scale\\": 1\\n  },\\n\\n  # \\"train-iters\\": 143000,\\n  \\"train-iters\\": 10000,\\n  \\"lr-decay-iters\\": 143000,\\n  \\"distributed-backend\\": \\"nccl\\",\\n  \\"lr-decay-style\\": \\"cosine\\",\\n  \\"warmup\\": 0.01,\\n  \\"checkpoint-factor\\": 1000,\\n  \\"extra-save-iters\\": [0,1,2,4,8,16,32,64,128,256,512],\\n  \\"eval-interval\\": 40000,\\n  \\"eval-iters\\": 10,\\n\\n  \\"log-interval\\": 10,\\n  \\"steps_per_print\\": 10,\\n  \\"wall_clock_breakdown\\": true,\\n\\n  # \\"train-data-paths\\": [\\"/fsx/pile_deduped/pile_0.87_deduped_text_document\\"],\\n  # \\"valid-data-paths\\": [\\"/fsx/pile_deduped/pile_0.87_deduped_text_document\\"],\\n  # \\"test-data-paths\\": [\\"/fsx/pile_deduped/pile_0.87_deduped_text_document\\"],\\n  \\"train-data-paths\\": [\\"/pythia_pile_idxmaps/pile_0.87_deduped_text_document\\"],\\n  \\"valid-data-paths\\": [\\"/pythia_pile_idxmaps/pile_0.87_deduped_text_document\\"],\\n  \\"test-data-paths\\": [\\"/pythia_pile_idxmaps/pile_0.87_deduped_text_document\\"],\\n\\n  \\"tokenizer-type\\": \\"HFTokenizer\\",\\n  \\"vocab-file\\": \\"/workspace/20B_tokenizer.json\\",\\n\\n  \\"launcher\\": \\"slurm\\",\\n  \\"deepspeed_slurm\\": false,\\n\\n  \\"save\\": \\"checkpoints/model_160m\\",\\n  \\"load\\": \\"checkpoints/model_160m\\",\\n  \\"checkpoint_validation_with_forward_pass\\": False,\\n}\\n"}, "load": "checkpoints/model_160m", "checkpoint_factor": 1000, "extra_save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512], "batch_size": 1, "train_iters": 10000, "eval_iters": 10, "eval_interval": 40000, "vocab_file": "/workspace/20B_tokenizer.json", "num_workers": 1, "attention_dropout": 0, "hidden_dropout": 0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 1, "clip_grad": 1.0, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "is_pipe_parallel": true, "wandb_group": "ZBQA5Wo9gxCEZn2i7LTWsv_muey74v1", "log_interval": 10, "text_gen_type": "unconditional", "user_script": "train.py", "save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000], "global_num_gpus": 1}']
===========================================================================
Traceback (most recent call last):
  File "deepy.py", line 40, in <module>
    sys.exit(5)
NameError: name 'sys' is not defined
